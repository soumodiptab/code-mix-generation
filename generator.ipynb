{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from cleantext import clean\n",
    "import os\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data \n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def replace_dates(text):\n",
    "        date_format_a = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', ' <DATE> ', text)\n",
    "        date_format_b = re.sub(\n",
    "            r'[A-Za-z]{2,8}\\s\\d{1,2},?\\s\\d {4}', ' <DATE> ', date_format_a)\n",
    "        date_format_c = re.sub(\n",
    "            r'\\d{2} [A-Z][a-z]{2,8} \\d{4}', ' <DATE> ', date_format_b)\n",
    "        return date_format_c\n",
    "\n",
    "def replace_concurrent_punctuation(text):\n",
    "    # replace concurrent punctuation with single punctuation\n",
    "    return re.sub(r'(!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|â€˜|\\{|\\||\\}|~){2,}', r' ', text)\n",
    "\n",
    "def replace_hash_tags(text):\n",
    "        return re.sub(r'(\\s|^)#(\\w+)', ' <HASHTAG> ', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "        # remove special characters other than punctuation\n",
    "        return re.sub(r'[^A-Za-z0-9\\s\\.\\,\\!\\?\\'\\\"\\:\\;]', ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "        return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "def replace_hyphenated_words(text):\n",
    "        # replace hyphenated words with words seperated by space\n",
    "        return re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', text)\n",
    "\n",
    "def read_data(filename, n_lines):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = []\n",
    "        for _ in range(n_lines):\n",
    "            line = f.readline().strip()\n",
    "            line = re.sub(r'<|>', ' ', line)\n",
    "            line = replace_dates(line)\n",
    "            line = replace_hyphenated_words(line)\n",
    "            line = replace_hash_tags(line)\n",
    "            # remove < and > from the text\n",
    "            line = clean(line, no_emoji=True,\n",
    "                         no_urls=True,\n",
    "                         no_emails=True,\n",
    "                         no_phone_numbers=True,\n",
    "                         no_currency_symbols=True,           \n",
    "                         replace_with_url=\" <URL> \",\n",
    "                         replace_with_email=\" <EMAIL> \",\n",
    "                         replace_with_phone_number=\" <PHONE> \",\n",
    "                         replace_with_currency_symbol=\" <CURRENCY> \",\n",
    "                         lower=True)\n",
    "            line = remove_special_characters(line)\n",
    "            #line = replace_concurrent_punctuation(line)\n",
    "            line = clean(line,no_numbers=True,no_digits=True,no_punct=True, replace_with_number=\" <NUMBER> \",replace_with_digit=\" \",replace_with_punct=\"\")\n",
    "            line = \"<BEGIN> \" + line + \" <END>\"\n",
    "            line = remove_extra_spaces(line)\n",
    "            tokens=tokenizer(line)\n",
    "            if len(tokens)>1:\n",
    "                lines.append(tokens)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_data(filename, lines):\n",
    "    # Save the data to a file\n",
    "    with open(filename, 'w')as f:\n",
    "        for line in lines:\n",
    "            line = ' '.join(line)\n",
    "            f.write(line.strip()+'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "\n",
    "data = read_data('data/alternate/L3Cube-HingCorpus_roman/R11_final_data/concatenated_train_final_shuffled.txt',20000)\n",
    "train,valid = train_test_split(data, test_size=0.3, random_state=42)\n",
    "valid,test=train_test_split(valid, test_size=0.5, random_state=42)\n",
    "#print(train[1:100])\n",
    "save_data('processed_data/train.txt', train)\n",
    "save_data('processed_data/valid.txt', valid)\n",
    "save_data('processed_data/test.txt', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L3CubeDataset(Dataset):\n",
    "    def __init__(self,filename,vocab=None,ngram=5):\n",
    "        data = self.read_data(filename)\n",
    "        if vocab is None:\n",
    "            self.vocab, self.ind2vocab = self.build_vocab(data)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.ind2vocab = {v:k for k,v in vocab.items()}\n",
    "        self.n = ngram\n",
    "        self.x,self.y = self.__create_dataset(data)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def read_data(self,filename):\n",
    "        lines = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.strip().split(' '))\n",
    "        return lines\n",
    "\n",
    "    def build_vocab(self,data):\n",
    "        word_set = set()\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set.add(word)\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        vocab_dict = {\"<unk>\":0}\n",
    "        for i,word in enumerate(word_list):\n",
    "            vocab_dict[word]=i+1\n",
    "        ind2word = {v:k for k,v in vocab_dict.items()}\n",
    "        return vocab_dict, ind2word\n",
    "    \n",
    "    def get_ngram(self, tokens):\n",
    "        n =self.n\n",
    "        ngram = []\n",
    "        if len(tokens) == 0:\n",
    "            return None\n",
    "        tokens = [\"<begin>\" for _ in range(n-2)] + tokens\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram.append(tokens[i:i+n])\n",
    "        return ngram\n",
    "    \n",
    "    def __get_seq(self, tokens):\n",
    "        vec= []\n",
    "        for word in tokens:\n",
    "            if word in self.vocab:\n",
    "                vec.append(self.vocab[word])\n",
    "            else:\n",
    "                vec.append(self.vocab[\"<unk>\"])\n",
    "        return vec\n",
    "\n",
    "    def __create_dataset(self, data):\n",
    "        x = []\n",
    "        y= []\n",
    "        ngrams = []\n",
    "        for line in data:\n",
    "            ngrams.extend(self.get_ngram(line))\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            x_tokens = ngram[:-1]\n",
    "            y_tokens = ngram[1:]\n",
    "            x.append(self.__get_seq(x_tokens))\n",
    "            y.append(self.__get_seq(y_tokens))\n",
    "        return torch.LongTensor(x),torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def get_dataloader(self, batch_size,shuffle=True):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = L3CubeDataset('processed_data/train.txt')\n",
    "validation_dataset = L3CubeDataset('processed_data/valid.txt',vocab=train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24011"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dump(train_dataset.get_vocab(),open('vocab.json','w'))\n",
    "len(train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aajtakabpnews\n",
      "aapdisappointing\n",
      "aayegaseekh\n",
      "abhicrackers\n",
      "accidentally\n",
      "accomplished\n",
      "accountability\n",
      "accountable\n",
      "achanakmaine\n",
      "achievement\n",
      "achievements\n",
      "acknowledge\n",
      "acknowledged\n",
      "acquaintance\n",
      "additionally\n",
      "adhikariyon\n",
      "administration\n",
      "administrative\n",
      "adversities\n",
      "advertisement\n",
      "advicekabhi\n",
      "aerodynamic\n",
      "affiliations\n",
      "afghanistan\n",
      "afternoonpartly\n",
      "agricultural\n",
      "agriculture\n",
      "alaipayuthey\n",
      "alhamdulillah\n",
      "allegations\n",
      "alnabilirdi\n",
      "alreadythen\n",
      "alternative\n",
      "alternatives\n",
      "amavaaskhairiyat\n",
      "anniversary\n",
      "announcement\n",
      "anthropocene\n",
      "anticipated\n",
      "anyonemaine\n",
      "apocalptica\n",
      "appearances\n",
      "appeasement\n",
      "application\n",
      "appointment\n",
      "appreciated\n",
      "appreciating\n",
      "appreciation\n",
      "apprhension\n",
      "appropriate\n",
      "appropriation\n",
      "approximately\n",
      "argumentative\n",
      "arrangement\n",
      "artificially\n",
      "assalamualaikumagar\n",
      "assassinated\n",
      "assignments\n",
      "association\n",
      "audiencehas\n",
      "authenticated\n",
      "authoritarian\n",
      "authorities\n",
      "automatically\n",
      "availability\n",
      "ayengekyounder\n",
      "ayudarlosmejor\n",
      "baatayeithey\n",
      "badhiyapure\n",
      "bajengeeeeeee\n",
      "balochistan\n",
      "banaobeuske\n",
      "bandaofficial\n",
      "banugitujhe\n",
      "bardhamaner\n",
      "baseerullah\n",
      "beautification\n",
      "beautifully\n",
      "beneficiaries\n",
      "benefitsstay\n",
      "berozgaarihealth\n",
      "bhaianyways\n",
      "bhaijaanoor\n",
      "bhaisahebisska\n",
      "bhaktonaagaya\n",
      "bhashachannd\n",
      "bhavishyasabko\n",
      "bhosadiwalon\n",
      "bhrashtachar\n",
      "biasedsalman\n",
      "bibibacchibahenammiapni\n",
      "bibliografa\n",
      "bigtymdynast\n",
      "billionaire\n",
      "biochemistry\n",
      "birthdayflop\n",
      "blackmailing\n",
      "blasphemous\n",
      "blessingslord\n",
      "blockbuster\n",
      "bolegiiilanat\n",
      "boltishould\n",
      "brahamastra\n",
      "brainslogics\n",
      "brainwashed\n",
      "broadcaster\n",
      "broadcasting\n",
      "brothalapathy\n",
      "brutalityif\n",
      "buddiesdont\n",
      "bulayabeech\n",
      "bureaucrats\n",
      "calculating\n",
      "calculation\n",
      "calculative\n",
      "calligrapher\n",
      "campaigning\n",
      "capabilities\n",
      "caterpillar\n",
      "celebrating\n",
      "celebratingto\n",
      "celebration\n",
      "celebrities\n",
      "centralised\n",
      "certificate\n",
      "certificates\n",
      "chahiyeafterall\n",
      "chahiyeboth\n",
      "chahiyeexcept\n",
      "chahiyeisse\n",
      "chahiyekamine\n",
      "chahiyepahle\n",
      "chahiyesana\n",
      "chahiyetrue\n",
      "chairmanship\n",
      "chairperson\n",
      "chakraborty\n",
      "chakravarty\n",
      "challengers\n",
      "challenging\n",
      "chamalalyaa\n",
      "championship\n",
      "chandrababu\n",
      "changesuske\n",
      "characteristics\n",
      "characterless\n",
      "chargesheet\n",
      "chargesheeted\n",
      "chattisgarh\n",
      "chhattisgarh\n",
      "chhodtephir\n",
      "childrenhave\n",
      "childrenplaying\n",
      "chillayenge\n",
      "cholesterol\n",
      "choreography\n",
      "christensen\n",
      "christianity\n",
      "chromasomes\n",
      "chromosomes\n",
      "chronically\n",
      "chrysanthemum\n",
      "chubihorlicksor\n",
      "chunnumunnupappu\n",
      "circumcision\n",
      "circumstances\n",
      "citiessuratbarodaamdavaad\n",
      "citizenship\n",
      "citizenswhy\n",
      "civilisation\n",
      "civilization\n",
      "clarification\n",
      "clarifications\n",
      "claustrophobia\n",
      "cleanliness\n",
      "clothsrudraksh\n",
      "cluelessffs\n",
      "coincidence\n",
      "collaborate\n",
      "collaboration\n",
      "collections\n",
      "combination\n",
      "comfortable\n",
      "comfortably\n",
      "commemorate\n",
      "commendable\n",
      "commentators\n",
      "commentsmaine\n",
      "commissioned\n",
      "commissioner\n",
      "commissionhats\n",
      "commissions\n",
      "commitments\n",
      "commodities\n",
      "communalism\n",
      "communication\n",
      "communities\n",
      "communityand\n",
      "compassionate\n",
      "compensated\n",
      "compensation\n",
      "competition\n",
      "competitive\n",
      "complaining\n",
      "complications\n",
      "complimenting\n",
      "compliments\n",
      "comprehension\n",
      "comprensora\n",
      "compromised\n",
      "comptroller\n",
      "concentrating\n",
      "condemnable\n",
      "conditional\n",
      "condolences\n",
      "confirmation\n",
      "confronting\n",
      "confusedavi\n",
      "congratulate\n",
      "congratulations\n",
      "connections\n",
      "connectivity\n",
      "connotation\n",
      "consciousness\n",
      "consequences\n",
      "consideration\n",
      "considerations\n",
      "considering\n",
      "consistency\n",
      "consistently\n",
      "consolidation\n",
      "constellation\n",
      "constipated\n",
      "constipation\n",
      "constituency\n",
      "constitution\n",
      "constitutionality\n",
      "constructing\n",
      "construction\n",
      "consultation\n",
      "contemplate\n",
      "contestants\n",
      "contingency\n",
      "continously\n",
      "continuously\n",
      "contradiction\n",
      "contradictions\n",
      "contribution\n",
      "contributor\n",
      "controversial\n",
      "controversy\n",
      "convenience\n",
      "conveniently\n",
      "conversation\n",
      "conversations\n",
      "convincingly\n",
      "cooperation\n",
      "coronaraseri\n",
      "corporation\n",
      "corporations\n",
      "correctitna\n",
      "correlation\n",
      "counselling\n",
      "countrywill\n",
      "courageously\n",
      "credentials\n",
      "credibility\n",
      "criminality\n",
      "culmination\n",
      "dakshineswar\n",
      "daylightanandapur\n",
      "decentralised\n",
      "decisionindia\n",
      "declaration\n",
      "defencehighest\n",
      "deforestation\n",
      "dekhegicheap\n",
      "dekhnakaran\n",
      "dekhoyaarharis\n",
      "deliberately\n",
      "demonetisation\n",
      "demonetization\n",
      "demonstrably\n",
      "demonstrated\n",
      "demonstrating\n",
      "demotivated\n",
      "derechizacin\n",
      "description\n",
      "descriptive\n",
      "desecration\n",
      "desestresantealtamente\n",
      "deshbhaktirastrabhakti\n",
      "desperately\n",
      "destabilize\n",
      "destruction\n",
      "determination\n",
      "devaluation\n",
      "development\n",
      "devffhnvafz\n",
      "dhanyawadapka\n",
      "dhritarashtra\n",
      "differences\n",
      "differentiate\n",
      "differentiation\n",
      "differently\n",
      "dikhayabaat\n",
      "directionless\n",
      "disabilities\n",
      "disagreeing\n",
      "disagreement\n",
      "disappointed\n",
      "disappointing\n",
      "disappointment\n",
      "discharging\n",
      "disconnected\n",
      "discrepancy\n",
      "discriminate\n",
      "discrimination\n",
      "discriminationopression\n",
      "discriminatory\n",
      "disgruntled\n",
      "disinvestment\n",
      "dispatchers\n",
      "disrespected\n",
      "disrespectful\n",
      "disruptinke\n",
      "dissertation\n",
      "distinction\n",
      "distraction\n",
      "distribution\n",
      "distributors\n",
      "dominatingchalati\n",
      "doordarshan\n",
      "dormirvuelvo\n",
      "downloading\n",
      "dramatically\n",
      "dronacharya\n",
      "dudaklarnda\n",
      "dununununununununununununun\n",
      "durvyavahaar\n",
      "dzenliyoruz\n",
      "easthampton\n",
      "economically\n",
      "effortlessly\n",
      "electionground\n",
      "electricity\n",
      "electronics\n",
      "elimination\n",
      "elternratgeberinnen\n",
      "embarrasment\n",
      "embarrassed\n",
      "embarrassing\n",
      "emotionally\n",
      "emotionswhy\n",
      "empowerment\n",
      "encouraging\n",
      "engineerand\n",
      "engineering\n",
      "enlightened\n",
      "enlightenment\n",
      "entertainer\n",
      "entertaining\n",
      "entertainment\n",
      "enthusiasmi\n",
      "entrenamiento\n",
      "entrepreneur\n",
      "entretenidsimoy\n",
      "environment\n",
      "environmentbhai\n",
      "epigoebbels\n",
      "episodeusne\n",
      "equilibrium\n",
      "established\n",
      "establishment\n",
      "ettkicksultantzhand\n",
      "eventuallyall\n",
      "everlasting\n",
      "exactlykuch\n",
      "exaggeration\n",
      "examination\n",
      "exceptional\n",
      "exclusively\n",
      "expatriates\n",
      "expectation\n",
      "expectations\n",
      "expenditure\n",
      "experienced\n",
      "experiences\n",
      "explanation\n",
      "explanations\n",
      "expressionfreedom\n",
      "expressions\n",
      "fadeawaynow\n",
      "familiarity\n",
      "farmerworldanimalworld\n",
      "fascinating\n",
      "fascistasporboric\n",
      "felicitaciones\n",
      "fenerasyona\n",
      "festivities\n",
      "fewdaysweakness\n",
      "firecrackers\n",
      "flexibility\n",
      "fodeinmachchar\n",
      "followershe\n",
      "foolishness\n",
      "foreshadowing\n",
      "forgetfulness\n",
      "forgiveness\n",
      "formalities\n",
      "fortification\n",
      "friendships\n",
      "friendsmtlb\n",
      "frustrating\n",
      "frustration\n",
      "frustrationi\n",
      "fundamental\n",
      "gangainathji\n",
      "gehrataarif\n",
      "generalhawa\n",
      "generational\n",
      "generations\n",
      "gestationnel\n",
      "gherauparliament\n",
      "ghumayaboht\n",
      "girengebhale\n",
      "girlfriends\n",
      "girlspropertybusinesslife\n",
      "goodbyeeeeeeeee\n",
      "governments\n",
      "grandfather\n",
      "grandiosity\n",
      "greatparmatma\n",
      "guerrillero\n",
      "gunslinging\n",
      "hahahahahaha\n",
      "hahahahhaha\n",
      "haideshdrohi\n",
      "haifrndshippyarsoft\n",
      "haimahjabeen\n",
      "hainviewership\n",
      "haipaisaipl\n",
      "haisadtujhe\n",
      "haitandoori\n",
      "haiuparwale\n",
      "handicapped\n",
      "happinesssave\n",
      "happyobviously\n",
      "hardworking\n",
      "harkatensab\n",
      "harshwardhan\n",
      "headquarters\n",
      "headwherever\n",
      "heyfavorism\n",
      "hichkichana\n",
      "hiconfident\n",
      "highlighted\n",
      "highlighting\n",
      "hindumuslim\n",
      "historiography\n",
      "hnselfishphlay\n",
      "hogarkvians\n",
      "hogasooryavanshi\n",
      "holistically\n",
      "honestydedication\n",
      "hoonabhitak\n",
      "hopakistanis\n",
      "hospitality\n",
      "hotaerghhjj\n",
      "hotisarkari\n",
      "humanitarian\n",
      "humarecontroversies\n",
      "hypocritical\n",
      "hypothetical\n",
      "ibrahimdaniels\n",
      "identification\n",
      "iiiiiiissssssssssssssssss\n",
      "imagination\n",
      "imlieplease\n",
      "immediately\n",
      "imperishable\n",
      "implementation\n",
      "implemented\n",
      "implementing\n",
      "implications\n",
      "importanceso\n",
      "importantwe\n",
      "impractical\n",
      "impressionable\n",
      "improvement\n",
      "improvements\n",
      "improvisation\n",
      "inaugurates\n",
      "inauguration\n",
      "incompetent\n",
      "incontinent\n",
      "increasedand\n",
      "increasedcooking\n",
      "increasedpetrol\n",
      "independence\n",
      "independent\n",
      "independently\n",
      "indestructible\n",
      "indiaheight\n",
      "indiaspeaks\n",
      "indigestion\n",
      "indisponerme\n",
      "individually\n",
      "individuals\n",
      "indomptable\n",
      "industriesbut\n",
      "industrycrazy\n",
      "inexperience\n",
      "inferiority\n",
      "inflammatory\n",
      "influencing\n",
      "informacioncreo\n",
      "informalidad\n",
      "information\n",
      "infrastructure\n",
      "infringement\n",
      "initiatives\n",
      "inkeoisliye\n",
      "inseminacio\n",
      "insensitive\n",
      "insignificant\n",
      "inspiration\n",
      "inspirational\n",
      "inspirations\n",
      "installment\n",
      "institution\n",
      "institutional\n",
      "institutionalized\n",
      "institutions\n",
      "instructions\n",
      "insurrection\n",
      "integration\n",
      "integrative\n",
      "intehaanpiya\n",
      "intellectual\n",
      "intelligence\n",
      "intelligent\n",
      "intentionally\n",
      "interacting\n",
      "interaction\n",
      "interactions\n",
      "interesting\n",
      "interestingguess\n",
      "interestingly\n",
      "international\n",
      "interpretation\n",
      "interpreted\n",
      "interrogation\n",
      "intervention\n",
      "interviewed\n",
      "interviewer\n",
      "intimidated\n",
      "intolerance\n",
      "introduction\n",
      "investedfrom\n",
      "investigate\n",
      "investigated\n",
      "investigating\n",
      "investigation\n",
      "investigations\n",
      "investments\n",
      "involvedwhere\n",
      "involvement\n",
      "irreplaceable\n",
      "irresponsible\n",
      "irukanggela\n",
      "islamophobia\n",
      "ityauauaujajakaoooo\n",
      "jaathajaisa\n",
      "jaayegashes\n",
      "jaayemausam\n",
      "jachemotherapy\n",
      "janmabhumistill\n",
      "jayegajitna\n",
      "jayegasharukh\n",
      "jayegawapis\n",
      "jayengeosme\n",
      "jeetogekuchh\n",
      "jindabadjay\n",
      "journalists\n",
      "journeyapni\n",
      "jubinboliyekya\n",
      "jungleakshu\n",
      "justifiable\n",
      "justification\n",
      "justifiedno\n",
      "kahapitamah\n",
      "kahengelogo\n",
      "kaisamantriyon\n",
      "kamsamasyao\n",
      "kanyakumari\n",
      "karegahindu\n",
      "karegapreesha\n",
      "kareinwanna\n",
      "karengelekin\n",
      "karkeschool\n",
      "karogikabhi\n",
      "karvaogehinduo\n",
      "karyapranali\n",
      "kashmiriyatj\n",
      "kehnaignore\n",
      "khansomething\n",
      "khanyounare\n",
      "khanypeenysony\n",
      "kharochedarr\n",
      "khelegiunhe\n",
      "khilaayengi\n",
      "khoobsoorat\n",
      "khruschowwho\n",
      "kickdumboleena\n",
      "kihmahengai\n",
      "knowledgeable\n",
      "kyunpopcorn\n",
      "laayisources\n",
      "ladaihungamamudda\n",
      "ladkifitrat\n",
      "lagayainphir\n",
      "laggingthats\n",
      "languagesrace\n",
      "lawlessness\n",
      "lecturingbtw\n",
      "legislation\n",
      "libranduits\n",
      "limpression\n",
      "liyebuniyaadi\n",
      "localisation\n",
      "lootedruined\n",
      "mahapurusho\n",
      "maharashtra\n",
      "mahjabeenso\n",
      "maintaining\n",
      "maintenance\n",
      "majoritarianism\n",
      "malatilakpooja\n",
      "manaatabumrah\n",
      "mangalsutra\n",
      "manifestation\n",
      "manifestations\n",
      "manifesting\n",
      "manipulated\n",
      "manipulating\n",
      "manipulation\n",
      "manipulative\n",
      "manufacturing\n",
      "margdarshan\n",
      "marginalised\n",
      "marketplace\n",
      "marvelously\n",
      "masterclass\n",
      "masterpiece\n",
      "masterstroke\n",
      "matchmaking\n",
      "mathematics\n",
      "meaningless\n",
      "measurement\n",
      "measurements\n",
      "mediavipaksh\n",
      "meetingsthe\n",
      "megalomania\n",
      "merchandising\n",
      "mercilessly\n",
      "merethankhs\n",
      "meritocracy\n",
      "microbiology\n",
      "milegachahe\n",
      "millennials\n",
      "millionaires\n",
      "mindthinking\n",
      "ministerial\n",
      "ministerios\n",
      "miraculously\n",
      "miscellaneous\n",
      "miscommunication\n",
      "misdirected\n",
      "mismanagement\n",
      "missionaries\n",
      "misunderstanding\n",
      "misunderstandings\n",
      "misunderstood\n",
      "mnopqrstuvw\n",
      "mohabbatein\n",
      "moisturiser\n",
      "monetisation\n",
      "monetization\n",
      "morningovercast\n",
      "morningstopped\n",
      "mosquechurch\n",
      "motherboard\n",
      "motivational\n",
      "motivationnel\n",
      "mulakaatein\n",
      "multitasking\n",
      "municipality\n",
      "nabadeykisai\n",
      "nationalism\n",
      "nationalist\n",
      "nationalists\n",
      "nationalplayers\n",
      "nbykwazfrbrznj\n",
      "necessarily\n",
      "needesadoption\n",
      "neighborhood\n",
      "neighbouring\n",
      "newsfirstpost\n",
      "nikaldimaag\n",
      "noidaforget\n",
      "nominations\n",
      "nonsensical\n",
      "normalisation\n",
      "normalization\n",
      "northampton\n",
      "northamptonshire\n",
      "nostradamus\n",
      "notificacins\n",
      "notification\n",
      "notifications\n",
      "objectionable\n",
      "objectively\n",
      "observation\n",
      "observations\n",
      "obviouslytony\n",
      "occasionally\n",
      "officertejaswi\n",
      "operational\n",
      "opportunist\n",
      "opportunity\n",
      "organisations\n",
      "organization\n",
      "organizations\n",
      "othersdosron\n",
      "outsourcing\n",
      "outstanding\n",
      "overconfidence\n",
      "overconfident\n",
      "overexcited\n",
      "overflowing\n",
      "overthinking\n",
      "overwhelmed\n",
      "overwhelming\n",
      "ovpgytwihtt\n",
      "padegaishwar\n",
      "pakistanbus\n",
      "pakistaniyo\n",
      "pandeybmcom\n",
      "parastisahi\n",
      "parliamentarian\n",
      "participant\n",
      "participants\n",
      "participate\n",
      "participated\n",
      "particularly\n",
      "partnership\n",
      "partnerships\n",
      "pathological\n",
      "patrakarita\n",
      "payengijust\n",
      "payetumhara\n",
      "perfectionist\n",
      "performance\n",
      "performances\n",
      "permanently\n",
      "permissions\n",
      "permutation\n",
      "personality\n",
      "personified\n",
      "perspective\n",
      "phirtagline\n",
      "photographers\n",
      "photography\n",
      "physiotherapy\n",
      "placesnobody\n",
      "planetarium\n",
      "playstation\n",
      "politicianive\n",
      "politicians\n",
      "populations\n",
      "porkistanis\n",
      "positionuske\n",
      "possibility\n",
      "possibilitysquared\n",
      "posthumously\n",
      "potentialhis\n",
      "prabhupdathrough\n",
      "practically\n",
      "pradhanmantri\n",
      "pratikhimmat\n",
      "precipitation\n",
      "predictable\n",
      "predictions\n",
      "preparation\n",
      "preparedness\n",
      "presentacin\n",
      "prestigious\n",
      "presumption\n",
      "previlegesbass\n",
      "privatization\n",
      "priyadarshan\n",
      "priyankamujhe\n",
      "probability\n",
      "problematic\n",
      "productivity\n",
      "professional\n",
      "professionally\n",
      "professionals\n",
      "progression\n",
      "progressive\n",
      "progrezombies\n",
      "pronouncing\n",
      "propogandacar\n",
      "protections\n",
      "protestedopenly\n",
      "prsidentielles\n",
      "prximamente\n",
      "psychiatrist\n",
      "puchegifake\n",
      "punctuation\n",
      "puncturewala\n",
      "purposejust\n",
      "pyaarcummacati\n",
      "qualifications\n",
      "questionable\n",
      "questionedbut\n",
      "questioning\n",
      "questionsheight\n",
      "quintessential\n",
      "radicalisation\n",
      "rahegaisliye\n",
      "rahengehumare\n",
      "raheramayanmahabharat\n",
      "rakshabandhan\n",
      "ramachandran\n",
      "ramakrishna\n",
      "ramcongratulations\n",
      "ramulooramulaa\n",
      "ravichandran\n",
      "reactionary\n",
      "reasonlittle\n",
      "recalentado\n",
      "reciprocate\n",
      "recognition\n",
      "recomendable\n",
      "recommendation\n",
      "recommendations\n",
      "recommended\n",
      "recommending\n",
      "recruitment\n",
      "recruitmentstudying\n",
      "reelspicsvlogs\n",
      "refrigerators\n",
      "refusedfinally\n",
      "regionalism\n",
      "registration\n",
      "rehabilitation\n",
      "relationship\n",
      "relationships\n",
      "relationshit\n",
      "religionislam\n",
      "religionwhere\n",
      "religiouslyhumanity\n",
      "remainwhich\n",
      "remembering\n",
      "renaissance\n",
      "repercussions\n",
      "replacement\n",
      "representation\n",
      "representative\n",
      "represented\n",
      "reproduction\n",
      "republicano\n",
      "reputationnill\n",
      "requirement\n",
      "requirements\n",
      "researchers\n",
      "reservation\n",
      "reservations\n",
      "resettlement\n",
      "resignation\n",
      "resolutions\n",
      "respectable\n",
      "respecteddil\n",
      "responsibality\n",
      "responsibilities\n",
      "responsibility\n",
      "responsible\n",
      "responsibly\n",
      "restrictions\n",
      "retaliation\n",
      "retroactivo\n",
      "returnsmaine\n",
      "revelations\n",
      "reverendsima\n",
      "rfsizlikdir\n",
      "rhinestones\n",
      "rightsdemocracyfreedom\n",
      "rishtedaars\n",
      "rozpieszczaj\n",
      "rudraprayag\n",
      "saafladkiyon\n",
      "saktesarkar\n",
      "samajavaragamana\n",
      "samasyamomin\n",
      "samecopying\n",
      "samjhimatlab\n",
      "samjigaliya\n",
      "saraastress\n",
      "sarcastically\n",
      "sarkaarbengal\n",
      "sathultimate\n",
      "satisfaction\n",
      "satisfactory\n",
      "saurabhtera\n",
      "scholarship\n",
      "secessionist\n",
      "secondlyhave\n",
      "secularkashmir\n",
      "selectively\n",
      "semiconductor\n",
      "sensibility\n",
      "sensitivity\n",
      "seriouslydes\n",
      "seriouslydost\n",
      "seriouslyhe\n",
      "seriouslyyeh\n",
      "sessizliinden\n",
      "sevdiklerine\n",
      "shahabuddin\n",
      "shamelessly\n",
      "sharmajihanuman\n",
      "sharmindagi\n",
      "showstopper\n",
      "shukralhumdulillah\n",
      "significant\n",
      "similarities\n",
      "simultaneously\n",
      "singledouble\n",
      "sobrenatural\n",
      "sochuhamari\n",
      "solizpintaba\n",
      "somasundarams\n",
      "soonleaving\n",
      "sooooorrrryyy\n",
      "sooryavanshi\n",
      "specifically\n",
      "spectacularly\n",
      "spinelessso\n",
      "spiritually\n",
      "splitsvilla\n",
      "spokesperson\n",
      "sportsperson\n",
      "standartlar\n",
      "statementif\n",
      "staubsauger\n",
      "sterilization\n",
      "stormbreaker\n",
      "storytelling\n",
      "straightening\n",
      "strengthened\n",
      "sttaemnents\n",
      "stylewalklaugh\n",
      "subcontinent\n",
      "subhanallah\n",
      "subramanian\n",
      "subscribers\n",
      "subscription\n",
      "substantial\n",
      "substantiate\n",
      "successfully\n",
      "suggestions\n",
      "superficial\n",
      "superheroics\n",
      "superiority\n",
      "supplementation\n",
      "surprisingly\n",
      "surrendered\n",
      "suryavanshi\n",
      "sustainable\n",
      "switzerland\n",
      "systematically\n",
      "tasallibhakto\n",
      "technicians\n",
      "tempaparent\n",
      "temperament\n",
      "temperature\n",
      "temptations\n",
      "terbangmereka\n",
      "termination\n",
      "thaadeegree\n",
      "thahonestly\n",
      "thaindirectly\n",
      "thangilmour\n",
      "thanksgiving\n",
      "themaelvesdoc\n",
      "theoretical\n",
      "theorgancview\n",
      "therapeutic\n",
      "therapyfattukuch\n",
      "threatening\n",
      "tirunelveli\n",
      "tmlgchamchagiri\n",
      "togetherness\n",
      "torchbearer\n",
      "tournaments\n",
      "traditionalist\n",
      "trafficking\n",
      "transaction\n",
      "transactions\n",
      "transferred\n",
      "transformation\n",
      "transformed\n",
      "translation\n",
      "transparente\n",
      "transpiration\n",
      "transversal\n",
      "traumatized\n",
      "travelingab\n",
      "trillionlets\n",
      "trollingwould\n",
      "trustworthy\n",
      "tuberculosis\n",
      "tumharifans\n",
      "tumhetumhare\n",
      "tweetthough\n",
      "unattractive\n",
      "unauthorised\n",
      "unbelievable\n",
      "unbornimaginary\n",
      "unbreakable\n",
      "uncertainty\n",
      "uncomfortable\n",
      "unconditional\n",
      "unconditionally\n",
      "underdeveloped\n",
      "underestimate\n",
      "underground\n",
      "understandable\n",
      "understanding\n",
      "understands\n",
      "understatement\n",
      "underworlds\n",
      "undeserving\n",
      "undoubtedly\n",
      "unemployment\n",
      "unforgettable\n",
      "unfortunately\n",
      "unhabitatyouth\n",
      "unimaginable\n",
      "unintentionally\n",
      "universefor\n",
      "unmanageable\n",
      "unnecessarily\n",
      "unnecessary\n",
      "unpatriotic\n",
      "unprecedented\n",
      "unprofessional\n",
      "unreligious\n",
      "unrighteousness\n",
      "unspeakable\n",
      "unstoppable\n",
      "unsuccessful\n",
      "unsustainability\n",
      "untouchable\n",
      "unvaccinated\n",
      "upbringingness\n",
      "uthaeyeimam\n",
      "uttarakhand\n",
      "vaccination\n",
      "valenciahopefully\n",
      "verification\n",
      "vickysharma\n",
      "victimhoodslavery\n",
      "videography\n",
      "videoslikes\n",
      "villagetheyre\n",
      "virubhaibad\n",
      "vishakhapatnam\n",
      "vishweshwar\n",
      "waterlogged\n",
      "wbgzzcqtdtxqahgfo\n",
      "westernized\n",
      "whataboutery\n",
      "whatsappobviously\n",
      "wicketkeeper\n",
      "wkwkwkwkwkjjj\n",
      "wordswhatever\n",
      "workoutalag\n",
      "worshippers\n",
      "worstghatiya\n",
      "wouldcriket\n",
      "yadavnileshkumar\n",
      "zarooribewakoofvellefaltubadakal\n",
      "zindabaaaaad\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = train_dataset.get_vocab()\n",
    "for word in vocab_dict:\n",
    "    if len(word) > 10:\n",
    "        print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,     1, 11688, 12520]) tensor([    1, 11688, 12520, 12030])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x[2],train_dataset.y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramNet(nn.Module):\n",
    "    def __init__(self,vocab_size, n_hidden=256, n_layers=4,embedding_dim=200, dropout=0.2, lr=0.001,device='cuda'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, n_hidden, n_layers, dropout=dropout,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)     \n",
    "        lstm_output, hidden = self.rnn(embedded, hidden)\n",
    "        out = self.dropout(lstm_output)\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def __init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def accuracy(self,true, pred):\n",
    "        true = np.array(true)\n",
    "        pred = np.array(pred)\n",
    "        num_correct = sum(true == pred)\n",
    "        num_total = len(true)\n",
    "        return num_correct / num_total\n",
    "\n",
    "\n",
    "    def run_training(self,train_dataset,valid_dataset, epochs=10, batch_size=32, clip = 1,print_every=1):\n",
    "        device = self.device\n",
    "        if str(device) == 'cpu':\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "        torch.cuda.empty_cache()\n",
    "        self.to(device)\n",
    "        train_loader = train_dataset.get_dataloader(batch_size)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            hidden = self.__init_hidden(batch_size)\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output, hidden = self.forward(x, hidden)\n",
    "                loss = criterion(output, y.view(-1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.parameters(), clip)\n",
    "                optimizer.step()\n",
    "            if i % print_every == 0:\n",
    "                acc,_ = self.evaluate(train_dataset)\n",
    "                acc2,_ = self.evaluate(valid_dataset)\n",
    "                self.train()\n",
    "                print(\"Epoch: {}/{}\".format(epoch+1, epochs),\n",
    "#                       \"Step: {}\".format(i),\n",
    "                      \"Loss: {}\".format(loss.item()),\n",
    "                      \"Training Accuracy: {}\".format(acc),\n",
    "                      \"Validation Accuracy: {}\".format(acc2))\n",
    "                    \n",
    "    def evaluate(self, dataset, batch_size=32):\n",
    "        device = self.device\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = dataset.get_dataloader(batch_size)\n",
    "        hidden = self.__init_hidden(batch_size)\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            x, y = x.to(device), y\n",
    "            output, hidden = self.forward(x, hidden)\n",
    "            preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "            trues.extend(y.view(-1).numpy())\n",
    "        accuracy = self.accuracy(trues, preds)\n",
    "        return accuracy, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GramNet(\n",
      "  (embedding): Embedding(24011, 200)\n",
      "  (rnn): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=24011, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = GramNet(len(train_dataset.get_vocab()))\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 527.54it/s]\n",
      "7it [00:04,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 0 Loss: 10.087209701538086 Validation Accuracy: 0.0013904963991769547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 496.54it/s]\n",
      "210it [00:12,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 200 Loss: 7.362147331237793 Validation Accuracy: 0.07203655478395062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 564.02it/s]\n",
      "407it [00:19,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 400 Loss: 7.489235877990723 Validation Accuracy: 0.07203253600823045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 549.80it/s]\n",
      "609it [00:27,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 600 Loss: 7.133872985839844 Validation Accuracy: 0.09249212319958848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 534.32it/s]\n",
      "807it [00:35,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 800 Loss: 7.121161937713623 Validation Accuracy: 0.09844393004115226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 493.67it/s]\n",
      "1005it [00:43,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step: 1000 Loss: 6.981566905975342 Validation Accuracy: 0.10263953189300412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1122it [00:46, 24.37it/s]\n",
      "1944it [00:03, 559.94it/s]\n",
      "7it [00:04,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 0 Loss: 7.664282321929932 Validation Accuracy: 0.10557323816872428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:04, 435.61it/s]\n",
      "205it [00:12,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 200 Loss: 6.6467509269714355 Validation Accuracy: 0.10907761059670781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 518.85it/s]\n",
      "407it [00:20,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 400 Loss: 6.788148880004883 Validation Accuracy: 0.11346611368312758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:03, 525.26it/s]\n",
      "610it [00:29,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 600 Loss: 6.586303234100342 Validation Accuracy: 0.11681375385802469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:04, 435.43it/s]\n",
      "809it [00:38,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 800 Loss: 6.546810626983643 Validation Accuracy: 0.11832079475308642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [00:04, 408.45it/s]\n",
      "1009it [00:47,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step: 1000 Loss: 6.3822126388549805 Validation Accuracy: 0.1206516846707819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1122it [00:49, 22.50it/s]\n"
     ]
    }
   ],
   "source": [
    "net.run_training(train_dataset,validation_dataset, epochs=2, batch_size=256, clip = 1,print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './saved_models/model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = WordLSTM(len(train_dataset.get_vocab()))\n",
    "net2.load_state_dict(torch.load('./saved_models/model_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perp(prob,n):\n",
    "  p = math.exp(prob*(1/n))\n",
    "  return 1/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(net, tkn, trg, h=None): \n",
    "    # tensor inputs\n",
    "    if tkn in token2int:\n",
    "      x = np.array([[token2int[tkn]]])\n",
    "    else:\n",
    "      x = np.array([[token2int['UNK']]])\n",
    "\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # push to GPU\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    if trg in token2int:\n",
    "        prob = p[token2int[trg]]\n",
    "    else:\n",
    "        prob = p[token2int['UNK']]\n",
    "\n",
    "    return h, prob\n",
    "# function to calculate perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(net, x, y):\n",
    "\n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    N = len(x)\n",
    "    tmp = 1\n",
    "\n",
    "    for i, gram in enumerate(x):\n",
    "        h, prob = predict_prob(net, gram, y[i], h)\n",
    "        if (i==0): \n",
    "          if (gram in word_counter): tmp*= word_counter[gram]/vocab_size\n",
    "          else: tmp*= ind/vocab_size   \n",
    "        else:\n",
    "          tmp *= prob\n",
    "        # print(gram,prob,tmp)\n",
    "\n",
    "    return math.log(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
