{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from cleantext import clean\n",
    "import os\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data \n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def replace_dates(text):\n",
    "        date_format_a = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', ' <DATE> ', text)\n",
    "        date_format_b = re.sub(\n",
    "            r'[A-Za-z]{2,8}\\s\\d{1,2},?\\s\\d {4}', ' <DATE> ', date_format_a)\n",
    "        date_format_c = re.sub(\n",
    "            r'\\d{2} [A-Z][a-z]{2,8} \\d{4}', ' <DATE> ', date_format_b)\n",
    "        return date_format_c\n",
    "\n",
    "def replace_concurrent_punctuation(text):\n",
    "    # replace concurrent punctuation with single punctuation\n",
    "    return re.sub(r'(!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|â€˜|\\{|\\||\\}|~){2,}', r' ', text)\n",
    "\n",
    "def replace_hash_tags(text):\n",
    "        return re.sub(r'(\\s|^)#(\\w+)', ' <HASHTAG> ', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "        # remove special characters other than punctuation\n",
    "        return re.sub(r'[^A-Za-z0-9\\s\\.\\,\\!\\?\\'\\\"\\:\\;]', ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "        return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "def replace_hyphenated_words(text):\n",
    "        # replace hyphenated words with words seperated by space\n",
    "        return re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', text)\n",
    "\n",
    "def read_data(filename, n_lines):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = []\n",
    "        for _ in range(n_lines):\n",
    "            line = f.readline().strip()\n",
    "            line = re.sub(r'<|>', ' ', line)\n",
    "            line = replace_dates(line)\n",
    "            line = replace_hyphenated_words(line)\n",
    "            line = replace_hash_tags(line)\n",
    "            # remove < and > from the text\n",
    "            line = clean(line, no_emoji=True,\n",
    "                         no_urls=True,\n",
    "                         no_emails=True,\n",
    "                         no_phone_numbers=True,\n",
    "                         no_currency_symbols=True,           \n",
    "                         replace_with_url=\" <URL> \",\n",
    "                         replace_with_email=\" <EMAIL> \",\n",
    "                         replace_with_phone_number=\" <PHONE> \",\n",
    "                         replace_with_currency_symbol=\" <CURRENCY> \",\n",
    "                         lower=True)\n",
    "            line = remove_special_characters(line)\n",
    "            #line = replace_concurrent_punctuation(line)\n",
    "            line = clean(line,no_numbers=True,no_digits=True,no_punct=True, replace_with_number=\" <NUMBER> \",replace_with_digit=\" \",replace_with_punct=\"\")\n",
    "            line = \"<BEGIN> \" + line + \" <END>\"\n",
    "            line = remove_extra_spaces(line)\n",
    "            tokens=tokenizer(line)\n",
    "            if len(tokens)>1:\n",
    "                lines.append(tokens)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_data(filename, lines):\n",
    "    # Save the data to a file\n",
    "    with open(filename, 'w')as f:\n",
    "        for line in lines:\n",
    "            line = ' '.join(line)\n",
    "            f.write(line.strip()+'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('./processed_data'):\n",
    "#     os.mkdir('processed_data')\n",
    "\n",
    "# data = read_data('data/alternate/L3Cube-HingCorpus_roman/R11_final_data/concatenated_train_final_shuffled.txt',20000)\n",
    "# train,valid = train_test_split(data, test_size=0.3, random_state=42)\n",
    "# valid,test=train_test_split(valid, test_size=0.5, random_state=42)\n",
    "# #print(train[1:100])\n",
    "# save_data('processed_data/train.txt', train)\n",
    "# save_data('processed_data/valid.txt', valid)\n",
    "# save_data('processed_data/test.txt', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L3CubeDataset(Dataset):\n",
    "    def __init__(self,filename,vocab=None,ngram=5):\n",
    "        data = self.read_data(filename)\n",
    "        if vocab is None:\n",
    "            self.vocab, self.ind2vocab = self.build_vocab(data)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.ind2vocab = {v:k for k,v in vocab.items()}\n",
    "        self.n = ngram\n",
    "        self.x,self.y = self.__create_dataset(data)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def read_data(self,filename):\n",
    "        lines = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.strip().split(' '))\n",
    "        return lines\n",
    "\n",
    "    def build_vocab(self,data):\n",
    "        word_set = set()\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set.add(word)\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        vocab_dict = {\"<unk>\":0}\n",
    "        for i,word in enumerate(word_list):\n",
    "            vocab_dict[word]=i+1\n",
    "        ind2word = {v:k for k,v in vocab_dict.items()}\n",
    "        return vocab_dict, ind2word\n",
    "    \n",
    "    def get_ngram(self, tokens):\n",
    "        n =self.n\n",
    "        ngram = []\n",
    "        if len(tokens) == 0:\n",
    "            return None\n",
    "        tokens = [\"<begin>\" for _ in range(n-2)] + tokens\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram.append(tokens[i:i+n])\n",
    "        return ngram\n",
    "    \n",
    "    def __get_seq(self, tokens):\n",
    "        vec= []\n",
    "        for word in tokens:\n",
    "            if word in self.vocab:\n",
    "                vec.append(self.vocab[word])\n",
    "            else:\n",
    "                vec.append(self.vocab[\"<unk>\"])\n",
    "        return vec\n",
    "\n",
    "    def __create_dataset(self, data):\n",
    "        x = []\n",
    "        y= []\n",
    "        ngrams = []\n",
    "        for line in data:\n",
    "            ngrams.extend(self.get_ngram(line))\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            x_tokens = ngram[:-1]\n",
    "            y_tokens = ngram[1:]\n",
    "            x.append(self.__get_seq(x_tokens))\n",
    "            y.append(self.__get_seq(y_tokens))\n",
    "        return torch.LongTensor(x),torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def get_dataloader(self, batch_size,shuffle=True):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LinceDataset(Dataset):\n",
    "    def __init__(self, filename, vocab_english=None, vocab_hinglish=None, ngram=5):\n",
    "        data_english, data_hinglish = self.read_data(filename)\n",
    "        if vocab_hinglish is None:\n",
    "            self.vocab_h, self.ind2vocab_h = self.build_vocab(data_hinglish)\n",
    "        else:\n",
    "            self.vocab_h = vocab_hinglish\n",
    "            self.ind2vocab_h = {v: k for k, v in vocab_hinglish.items()}\n",
    "        self.n = ngram\n",
    "        self.x, self.y = self.__create_dataset(data_hinglish)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab_h\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        english = []\n",
    "        hinglish = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                e = line.strip().split('\\t')[0]\n",
    "                english.append(e.strip().split(' '))\n",
    "                try:\n",
    "                    h = line.strip().split('\\t')[1]\n",
    "                except:\n",
    "                    h = \"\"\n",
    "                hinglish.append(h.strip().split(' '))\n",
    "        return english, hinglish\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        word_set = set()\n",
    "        for line in data:\n",
    "            for word in line:\n",
    "                if word not in word_set:\n",
    "                    word_set.add(word)\n",
    "        # sort the vocab\n",
    "        word_list = sorted(list(word_set))\n",
    "        vocab_dict = {\"<unk>\": 0}\n",
    "        for i, word in enumerate(word_list):\n",
    "            vocab_dict[word] = i+1\n",
    "        ind2word = {v: k for k, v in vocab_dict.items()}\n",
    "        return vocab_dict, ind2word\n",
    "\n",
    "    def get_ngram(self, tokens):\n",
    "        n = self.n\n",
    "        ngram = []\n",
    "        if len(tokens) == 0:\n",
    "            return None\n",
    "        tokens = [\"<begin>\" for _ in range(n-2)] + tokens\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram.append(tokens[i:i+n])\n",
    "        return ngram\n",
    "\n",
    "    def __get_seq(self, tokens):\n",
    "        vec = []\n",
    "        for word in tokens:\n",
    "            if word in self.vocab_h:\n",
    "                vec.append(self.vocab_h[word])\n",
    "            else:\n",
    "                vec.append(self.vocab_h[\"<unk>\"])\n",
    "        return vec\n",
    "\n",
    "    def __create_dataset(self, data):\n",
    "        x = []\n",
    "        y = []\n",
    "        ngrams = []\n",
    "        for line in data:\n",
    "            ngrams.extend(self.get_ngram(line))\n",
    "\n",
    "        for ngram in ngrams:\n",
    "            x_tokens = ngram[:-1]\n",
    "            y_tokens = ngram[1:]\n",
    "            x.append(self.__get_seq(x_tokens))\n",
    "            y.append(self.__get_seq(y_tokens))\n",
    "        return torch.LongTensor(x), torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def get_dataloader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LinceDataset('processed_data/lince/train.txt')\n",
    "validation_dataset = LinceDataset('processed_data/lince/valid.txt',vocab_hinglish=train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = L3CubeDataset('processed_data/train.txt')\n",
    "# validation_dataset = L3CubeDataset('processed_data/valid.txt',vocab=train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9237"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dump(train_dataset.get_vocab(),open('vocab.json','w'))\n",
    "len(train_dataset.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aakhirakaar\n",
      "aalochanaatmak\n",
      "aanandadaayak\n",
      "aashcharyachakit\n",
      "aashcharyajanak\n",
      "aatmavishvaas\n",
      "aavashyakata\n",
      "acceidentally\n",
      "accidentally\n",
      "accomplished\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = train_dataset.get_vocab()\n",
    "i=0\n",
    "for word in vocab_dict:\n",
    "    if i == 10:\n",
    "        break\n",
    "    if len(word) > 10:\n",
    "        print(word)\n",
    "        i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,    1,  947, 8904]) tensor([   1,  947, 8904, 8052])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x[2],train_dataset.y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramNet(nn.Module):\n",
    "    def __init__(self,vocab_size, n_hidden=256, n_layers=4,embedding_dim=200, dropout=None, lr=0.001,model_save_path='.',device='cuda'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.model_save_path = model_save_path\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if dropout is not None:\n",
    "            self.rnn = nn.LSTM(embedding_dim, n_hidden, n_layers, dropout=dropout,batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(embedding_dim, n_hidden, n_layers,batch_first=True)\n",
    "            dropout = 0\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "        self.model_name = 'GramNet_'+str(n_hidden)+'_'+str(n_layers)+'_'+str(dropout)+'_'+str(lr)+'.pt'\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)     \n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "#         out = self.dropout(out)\n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def __init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def accuracy(self,true, pred):\n",
    "        true = np.array(true)\n",
    "        pred = np.array(pred)\n",
    "        num_correct = sum(true == pred)\n",
    "        num_total = len(true)\n",
    "        return num_correct / num_total\n",
    "\n",
    "\n",
    "    def run_training(self,train_dataset,valid_dataset, epochs=10, batch_size=32, clip = 1,print_every=1):\n",
    "        device = self.device\n",
    "        if str(device) == 'cpu':\n",
    "            print(\"Training only supported in GPU environment\")\n",
    "            return\n",
    "        torch.cuda.empty_cache()\n",
    "        self.to(device)\n",
    "        train_loader = train_dataset.get_dataloader(batch_size)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            hidden = self.__init_hidden(batch_size)\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output, hidden = self.forward(x, hidden)\n",
    "                loss = criterion(output, y.view(-1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                clip_grad_norm_(self.parameters(), clip)\n",
    "                optimizer.step()\n",
    "            if i % print_every == 0:\n",
    "                acc,_ = self.evaluate(train_dataset)\n",
    "                acc2,_ = self.evaluate(valid_dataset)\n",
    "                self.train()\n",
    "                print(\"Epoch: {}/{}\".format(epoch+1, epochs),\n",
    "#                       \"Step: {}\".format(i),\n",
    "                      \"Loss: {}\".format(loss.item()),\n",
    "                      \"Training Accuracy: {}\".format(acc),\n",
    "                      \"Validation Accuracy: {}\".format(acc2))\n",
    "        self.save(os.path.join(self.model_save_path,self.model_name))\n",
    "                    \n",
    "    def evaluate(self, dataset, batch_size=32):\n",
    "        device = self.device\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = dataset.get_dataloader(batch_size)\n",
    "        hidden = self.__init_hidden(batch_size)\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            x, y = x.to(device), y\n",
    "            output, hidden = self.forward(x, hidden)\n",
    "            preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "            trues.extend(y.view(-1).numpy())\n",
    "        accuracy = self.accuracy(trues, preds)\n",
    "        return accuracy, preds\n",
    "    \n",
    "    def save(self,filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "    def load(self,filename):\n",
    "        self.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GramNet(\n",
      "  (embedding): Embedding(9237, 200)\n",
      "  (rnn): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=512, out_features=9237, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size =len(train_dataset.get_vocab())\n",
    "net = GramNet(vocab_size,512,3,200,0.2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Loss: 5.076810836791992 Training Accuracy: 0.21709235252140818 Validation Accuracy: 0.19979408914728683\n",
      "Epoch: 2/5 Loss: 4.5573906898498535 Training Accuracy: 0.26259217411988584 Validation Accuracy: 0.216953326873385\n",
      "Epoch: 3/5 Loss: 4.188779354095459 Training Accuracy: 0.30789624960355216 Validation Accuracy: 0.21816456718346253\n",
      "Epoch: 4/5 Loss: 3.718381643295288 Training Accuracy: 0.34990733032032983 Validation Accuracy: 0.2217377260981912\n",
      "Epoch: 5/5 Loss: 3.673319101333618 Training Accuracy: 0.38852878211227404 Validation Accuracy: 0.2197593669250646\n"
     ]
    }
   ],
   "source": [
    "net.run_training(train_dataset,validation_dataset, epochs=5, batch_size=64, clip = 1,print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './saved_models/model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = WordLSTM(len(train_dataset.get_vocab()))\n",
    "net2.load_state_dict(torch.load('./saved_models/model_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perp(prob,n):\n",
    "  p = math.exp(prob*(1/n))\n",
    "  return 1/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(net, tkn, trg, h=None): \n",
    "    # tensor inputs\n",
    "    if tkn in token2int:\n",
    "      x = np.array([[token2int[tkn]]])\n",
    "    else:\n",
    "      x = np.array([[token2int['UNK']]])\n",
    "\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # push to GPU\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    if trg in token2int:\n",
    "        prob = p[token2int[trg]]\n",
    "    else:\n",
    "        prob = p[token2int['UNK']]\n",
    "\n",
    "    return h, prob\n",
    "# function to calculate perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(net, x, y):\n",
    "\n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    N = len(x)\n",
    "    tmp = 1\n",
    "\n",
    "    for i, gram in enumerate(x):\n",
    "        h, prob = predict_prob(net, gram, y[i], h)\n",
    "        if (i==0): \n",
    "          if (gram in word_counter): tmp*= word_counter[gram]/vocab_size\n",
    "          else: tmp*= ind/vocab_size   \n",
    "        else:\n",
    "          tmp *= prob\n",
    "        # print(gram,prob,tmp)\n",
    "\n",
    "    return math.log(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
